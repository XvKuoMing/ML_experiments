{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVJVL0Ik82PK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0lC3Dd8w_qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data set\n",
        "- clean sentences by removing special characters\n",
        "- Add a start and end token to each sentence\n",
        "- create a word index and reverse word index\n",
        "- Pad each sentence to a maximum length"
      ],
      "metadata": {
        "id": "xArRcUr-AxnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the file\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "print(path_to_zip)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip) + \"/spa-eng/spa.txt\"\n",
        "print(path_to_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPznwqjOAvER",
        "outputId": "9c256b53-a461-483f-f164-2b43c577e4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n",
            "/root/.keras/datasets/spa-eng.zip\n",
            "/root/.keras/datasets/spa-eng/spa.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing\n",
        "#1. convert unicode files to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "  #creating a space between a word and the punctuation following it\n",
        "  # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r\"([.,!?¿])\", r\" \\1 \", w)\n",
        "  w = re.sub('\\s{2,}', ' ', w)\n",
        "\n",
        "  #replacing everything with space, except letters, punctuations ...\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  w = w.strip()\n",
        "\n",
        "  #adding start and end tokens\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "metadata": {
        "id": "rs4pqdatFB8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])\n",
        "print(len(en), len(sp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j7Wt_lSTnTJ",
        "outputId": "3c7a39ba-728b-4db0-db6d-45377ac1d348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n",
            "118964 118964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the sentence and pad the sequence to the same length\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "47chgw-1quQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "7E75YgkGuQil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2GEb-ooq_3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "print(max_length_targ, max_length_inp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlopqKyDuwKL",
        "outputId": "0ea20cfe-2cd1-48e3-f63b-e29a9cd8d04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
        "print(input_tensor_train[0])\n",
        "print(target_tensor_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HRObqP7vPhm",
        "outputId": "2bb9d08d-36a7-4301-eee5-51e002d79235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24000 24000 6000 6000\n",
            "[  1  25   8   7 557   3   2   0   0   0   0   0   0   0   0   0]\n",
            "[  1  27  11  34   9 466   3   2   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a tf.data dataset\n",
        "\n",
        "- create a source dataset from your input data\n",
        "- Apply dataset transformations to preprocess the data\n",
        "- Iterate over the dataset and process the elements\n",
        "\n",
        "\n",
        "Iteration occurs in streaming fashion, so the full dataset does not need to fit into memory"
      ],
      "metadata": {
        "id": "wjIu-jQxxA7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#configuration\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "steps_per_epoch_val = len(input_tensor_val)//BATCH_SIZE\n",
        "embedding_dim = 256 #for word embeddings\n",
        "units = 1024 ##dimensionality of the output space of the RNN\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch  = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5qdQFQFxO3z",
        "outputId": "8f90d1dc-10a8-4092-ce7d-e58a23269507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jr4BUe5uzRAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making the basic seq2seq model"
      ],
      "metadata": {
        "id": "NQacgPNRU_IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "E-fFi5brU_g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjICrKTc1b19",
        "outputId": "ccab0b39-8fe2-4c43-c280-4887d48ca8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    print(\"X:\", x.shape)\n",
        "    x = self.embedding(x)\n",
        "    print(\"XE:\", x.shape)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    print(\"A:\", output.shape)\n",
        "    print(\"B:\", state.shape)\n",
        "    output = tf.reshape(output, (-1,output.shape[2]))\n",
        "    print(\"C:\", output.shape)\n",
        "    x = self.fc(output)\n",
        "    print(\"D:\", x.shape)\n",
        "    return x,state"
      ],
      "metadata": {
        "id": "XGoqWRFg78l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE,1)), sample_hidden)\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j2SzW5JdbhD",
        "outputId": "7c026ddc-c141-479f-ee51-7508f842b48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: (64, 1)\n",
            "XE: (64, 1, 256)\n",
            "A: (64, 1, 1024)\n",
            "B: (64, 1024)\n",
            "C: (64, 1024)\n",
            "D: (64, 4935)\n",
            "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "olKH8Lu-d32M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dot product attention"
      ],
      "metadata": {
        "id": "PHbkATRVjpvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,units):\n",
        "    super(DotProductAttention,self).__init__()\n",
        "    self.WK = tf.keras.layers.Dense(units)\n",
        "    self.WQ= tf.keras.layers.Dense(units)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    #query -> s\n",
        "    #values -> h1 .. hn\n",
        "    query_with_time_axis = tf.expand_dims(query,1)\n",
        "\n",
        "    K = self.WK(values)\n",
        "    Q = self.WQ(query_with_time_axis)\n",
        "    QT = tf.einsum('ijk->ikj',Q)\n",
        "    score = tf.matmul(K, QT)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score,axis=1)\n",
        "\n",
        "    context_vector = attention_weights*values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "resD6nxL85Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = DotProductAttention(units)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svWE6K3LlhNU",
        "outputId": "34bad8f1-0c67-4ebb-bf74-636a402b208c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTC0_MppyOr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additive Attention"
      ],
      "metadata": {
        "id": "fHfAPj34z261"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,units):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2= tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self,query, values):\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score,axis=1)\n",
        "\n",
        "    context_vector = attention_weights*values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "gGuy3kRlz4h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kP7fmY4W1bTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder with Attention"
      ],
      "metadata": {
        "id": "HTMysVyi9vIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderWithAttention(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention=None):\n",
        "    super(DecoderWithAttention, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = attention\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    x = self.embedding(x)\n",
        "    attention_weights=None\n",
        "\n",
        "    if self.attention:\n",
        "      context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "      x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    output = tf.reshape(output, (-1,output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x,state,attention_weights"
      ],
      "metadata": {
        "id": "KWIsp6gq9xH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "US8CYPbHAsyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss function"
      ],
      "metadata": {
        "id": "qxBJ2RAOVjts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "def loss_function(real,pred):\n",
        "  loss_ = loss_object(real,pred)\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "GcCYXVd8Vln-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_object([1,2],[[0,0.6,0.3,0.1],[0,0.6,0.3,0.1]]))\n",
        "print(loss_function([1,2],[[0,0.6,0.3,0.1],[0,0.6,0.3,0.1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgm0dSSsWAMW",
        "outputId": "a81e0b54-00f5-4d1c-885c-b3e489945447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1.063386  1.3633859], shape=(2,), dtype=float32)\n",
            "tf.Tensor(1.2133859, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxKzLQpDWDlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "euiqo5qRW9rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def get_train_step_function():\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(inp, targ, enc_hidden, encoder, decoder):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "      dec_hidden = enc_hidden\n",
        "\n",
        "      dec_input = tf.expand_dims([targ_lang.word_index['<start>']]*BATCH_SIZE, 1)\n",
        "\n",
        "      for t in range(1,targ.shape[1]):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "        dec_input = tf.expand_dims(targ[:,t],1)\n",
        "\n",
        "    batch_loss = (loss/int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "  return train_step"
      ],
      "metadata": {
        "id": "Nq_x-D6iXM8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_validation_loss(inp, targ, enc_hidden, encoder, decoder):\n",
        "  loss = 0\n",
        "  enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']]*BATCH_SIZE,1)\n",
        "\n",
        "  for t in range(1, targ.shape[1]):\n",
        "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "    loss+=loss_function(targ[:,t], predictions)\n",
        "    dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  loss = loss/int(targ.shape[1])\n",
        "  return loss"
      ],
      "metadata": {
        "id": "G4SLY2UMezfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_seq2seq(epochs,attention):\n",
        "  encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "  decoder = DecoderWithAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention)\n",
        "  train_step_func = get_train_step_function()\n",
        "  training_loss = []\n",
        "  validation_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch,(inp,targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      batch_loss = train_step_func(inp, targ, enc_hidden, encoder, decoder)\n",
        "      total_loss+=batch_loss\n",
        "\n",
        "      if batch%100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(validation_dataset.take(steps_per_epoch_val)):\n",
        "      val_loss = calculate_validation_loss(inp, targ, enc_hidden, encoder, decoder)\n",
        "      total_val_loss += val_loss\n",
        "\n",
        "    training_loss.append(total_loss/steps_per_epoch)\n",
        "    validation_loss.append(total_val_loss/steps_per_epoch_val)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f} Validation Loss {:.4f}'.format(epoch + 1, training_loss[-1], validation_loss[-1]))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "  return encoder, decoder, training_loss, validation_loss"
      ],
      "metadata": {
        "id": "ib9guu-DjhEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ykn7FFZntaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training seq2seq without attention"
      ],
      "metadata": {
        "id": "mK51jHXUnuq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "attention = None\n",
        "print(\"Running seq2seq model without attention\")\n",
        "encoder, decoder, training_loss, validation_loss = training_seq2seq(epochs, attention)\n",
        "\n",
        "tloss = training_loss\n",
        "vloss = validation_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm6XPVkonxgL",
        "outputId": "c7beca4e-9740-43d0-e566-f37ff806a614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running seq2seq model without attention\n",
            "Epoch 1 Batch 0 Loss 7.7318\n",
            "Epoch 1 Batch 100 Loss 2.1228\n",
            "Epoch 1 Batch 200 Loss 1.9890\n",
            "Epoch 1 Batch 300 Loss 1.7133\n",
            "Epoch 1 Loss 2.0923 Validation Loss 1.6123\n",
            "Time taken for 1 epoch 65.80152082443237 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4999\n",
            "Epoch 2 Batch 100 Loss 1.5323\n",
            "Epoch 2 Batch 200 Loss 1.4634\n",
            "Epoch 2 Batch 300 Loss 1.3660\n",
            "Epoch 2 Loss 1.4199 Validation Loss 1.3065\n",
            "Time taken for 1 epoch 43.83407735824585 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1820\n",
            "Epoch 3 Batch 100 Loss 1.1052\n",
            "Epoch 3 Batch 200 Loss 1.1835\n",
            "Epoch 3 Batch 300 Loss 1.0444\n",
            "Epoch 3 Loss 1.0788 Validation Loss 1.1353\n",
            "Time taken for 1 epoch 43.41390919685364 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8849\n",
            "Epoch 4 Batch 100 Loss 0.8237\n",
            "Epoch 4 Batch 200 Loss 0.8856\n",
            "Epoch 4 Batch 300 Loss 0.8028\n",
            "Epoch 4 Loss 0.8394 Validation Loss 1.0241\n",
            "Time taken for 1 epoch 43.49695420265198 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6109\n",
            "Epoch 5 Batch 100 Loss 0.6058\n",
            "Epoch 5 Batch 200 Loss 0.7444\n",
            "Epoch 5 Batch 300 Loss 0.6671\n",
            "Epoch 5 Loss 0.6419 Validation Loss 0.9616\n",
            "Time taken for 1 epoch 44.33630657196045 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.4004\n",
            "Epoch 6 Batch 100 Loss 0.4690\n",
            "Epoch 6 Batch 200 Loss 0.5414\n",
            "Epoch 6 Batch 300 Loss 0.5316\n",
            "Epoch 6 Loss 0.4759 Validation Loss 0.9209\n",
            "Time taken for 1 epoch 43.53739047050476 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3059\n",
            "Epoch 7 Batch 100 Loss 0.3678\n",
            "Epoch 7 Batch 200 Loss 0.2749\n",
            "Epoch 7 Batch 300 Loss 0.3319\n",
            "Epoch 7 Loss 0.3428 Validation Loss 0.9032\n",
            "Time taken for 1 epoch 43.193679332733154 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2290\n",
            "Epoch 8 Batch 100 Loss 0.2354\n",
            "Epoch 8 Batch 200 Loss 0.2444\n",
            "Epoch 8 Batch 300 Loss 0.2574\n",
            "Epoch 8 Loss 0.2416 Validation Loss 0.8973\n",
            "Time taken for 1 epoch 43.38964366912842 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1164\n",
            "Epoch 9 Batch 100 Loss 0.1413\n",
            "Epoch 9 Batch 200 Loss 0.1575\n",
            "Epoch 9 Batch 300 Loss 0.1881\n",
            "Epoch 9 Loss 0.1716 Validation Loss 0.9161\n",
            "Time taken for 1 epoch 43.51075506210327 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1312\n",
            "Epoch 10 Batch 100 Loss 0.1295\n",
            "Epoch 10 Batch 200 Loss 0.1172\n",
            "Epoch 10 Batch 300 Loss 0.1448\n",
            "Epoch 10 Loss 0.1251 Validation Loss 0.9161\n",
            "Time taken for 1 epoch 43.294413566589355 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0604\n",
            "Epoch 11 Batch 100 Loss 0.0794\n",
            "Epoch 11 Batch 200 Loss 0.0803\n",
            "Epoch 11 Batch 300 Loss 0.1195\n",
            "Epoch 11 Loss 0.0959 Validation Loss 0.9355\n",
            "Time taken for 1 epoch 43.33020281791687 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0827\n",
            "Epoch 12 Batch 100 Loss 0.0714\n",
            "Epoch 12 Batch 200 Loss 0.0820\n",
            "Epoch 12 Batch 300 Loss 0.1024\n",
            "Epoch 12 Loss 0.0794 Validation Loss 0.9531\n",
            "Time taken for 1 epoch 43.36953377723694 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0508\n",
            "Epoch 13 Batch 100 Loss 0.0844\n",
            "Epoch 13 Batch 200 Loss 0.1050\n",
            "Epoch 13 Batch 300 Loss 0.0646\n",
            "Epoch 13 Loss 0.0687 Validation Loss 0.9599\n",
            "Time taken for 1 epoch 43.49831461906433 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0363\n",
            "Epoch 14 Batch 100 Loss 0.0873\n",
            "Epoch 14 Batch 200 Loss 0.0643\n",
            "Epoch 14 Batch 300 Loss 0.0739\n",
            "Epoch 14 Loss 0.0637 Validation Loss 0.9742\n",
            "Time taken for 1 epoch 43.33318376541138 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0365\n",
            "Epoch 15 Batch 100 Loss 0.0663\n",
            "Epoch 15 Batch 200 Loss 0.0507\n",
            "Epoch 15 Batch 300 Loss 0.0882\n",
            "Epoch 15 Loss 0.0616 Validation Loss 0.9942\n",
            "Time taken for 1 epoch 43.26407861709595 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0413\n",
            "Epoch 16 Batch 100 Loss 0.0429\n",
            "Epoch 16 Batch 200 Loss 0.0605\n",
            "Epoch 16 Batch 300 Loss 0.0622\n",
            "Epoch 16 Loss 0.0598 Validation Loss 0.9905\n",
            "Time taken for 1 epoch 43.2647340297699 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0496\n",
            "Epoch 17 Batch 100 Loss 0.0943\n",
            "Epoch 17 Batch 200 Loss 0.0306\n",
            "Epoch 17 Batch 300 Loss 0.0860\n",
            "Epoch 17 Loss 0.0599 Validation Loss 1.0158\n",
            "Time taken for 1 epoch 50.67096734046936 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0397\n",
            "Epoch 18 Batch 100 Loss 0.0404\n",
            "Epoch 18 Batch 200 Loss 0.0760\n",
            "Epoch 18 Batch 300 Loss 0.0509\n",
            "Epoch 18 Loss 0.0605 Validation Loss 1.0095\n",
            "Time taken for 1 epoch 43.525233030319214 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0427\n",
            "Epoch 19 Batch 100 Loss 0.0271\n",
            "Epoch 19 Batch 200 Loss 0.0512\n",
            "Epoch 19 Batch 300 Loss 0.0765\n",
            "Epoch 19 Loss 0.0590 Validation Loss 1.0191\n",
            "Time taken for 1 epoch 43.44853210449219 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0156\n",
            "Epoch 20 Batch 100 Loss 0.0814\n",
            "Epoch 20 Batch 200 Loss 0.0332\n",
            "Epoch 20 Batch 300 Loss 0.0484\n",
            "Epoch 20 Loss 0.0570 Validation Loss 1.0307\n",
            "Time taken for 1 epoch 43.59282684326172 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention = DotProductAttention(units)\n",
        "print(\"Running seq2seq model with dot product attention\")\n",
        "encoder_dp, decoder_dp, training_loss, validation_loss = training_seq2seq(epochs, attention)\n",
        "\n",
        "tloss = np.vstack((tloss, training_loss))\n",
        "vloss = np.vstack((vloss, validation_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otb-xsC0n_nh",
        "outputId": "a4c2ce9a-9734-4169-b754-e3c544b3400d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running seq2seq model with dot product attention\n",
            "Epoch 1 Batch 0 Loss 7.7311\n",
            "Epoch 1 Batch 100 Loss 2.5156\n",
            "Epoch 1 Batch 200 Loss 1.8885\n",
            "Epoch 1 Batch 300 Loss 1.8738\n",
            "Epoch 1 Loss 2.4150 Validation Loss 1.8649\n",
            "Time taken for 1 epoch 94.58045315742493 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.8512\n",
            "Epoch 2 Batch 100 Loss 1.5898\n",
            "Epoch 2 Batch 200 Loss 1.4620\n",
            "Epoch 2 Batch 300 Loss 1.5177\n",
            "Epoch 2 Loss 1.5848 Validation Loss 1.5170\n",
            "Time taken for 1 epoch 77.13783073425293 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3080\n",
            "Epoch 3 Batch 100 Loss 1.2058\n",
            "Epoch 3 Batch 200 Loss 1.3574\n",
            "Epoch 3 Batch 300 Loss 1.2462\n",
            "Epoch 3 Loss 1.2923 Validation Loss 1.3447\n",
            "Time taken for 1 epoch 77.12297630310059 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0964\n",
            "Epoch 4 Batch 100 Loss 1.0699\n",
            "Epoch 4 Batch 200 Loss 1.2635\n",
            "Epoch 4 Batch 300 Loss 1.0615\n",
            "Epoch 4 Loss 1.0916 Validation Loss 1.2427\n",
            "Time taken for 1 epoch 77.53688836097717 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.8499\n",
            "Epoch 5 Batch 100 Loss 1.0291\n",
            "Epoch 5 Batch 200 Loss 0.8472\n",
            "Epoch 5 Batch 300 Loss 0.9055\n",
            "Epoch 5 Loss 0.9220 Validation Loss 1.1415\n",
            "Time taken for 1 epoch 76.90393733978271 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.8567\n",
            "Epoch 6 Batch 100 Loss 0.8092\n",
            "Epoch 6 Batch 200 Loss 0.8513\n",
            "Epoch 6 Batch 300 Loss 0.7631\n",
            "Epoch 6 Loss 0.7803 Validation Loss 1.0734\n",
            "Time taken for 1 epoch 77.01518058776855 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6167\n",
            "Epoch 7 Batch 100 Loss 0.6912\n",
            "Epoch 7 Batch 200 Loss 0.7222\n",
            "Epoch 7 Batch 300 Loss 0.6443\n",
            "Epoch 7 Loss 0.6621 Validation Loss 1.0358\n",
            "Time taken for 1 epoch 76.83813381195068 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5718\n",
            "Epoch 8 Batch 100 Loss 0.5438\n",
            "Epoch 8 Batch 200 Loss 0.5121\n",
            "Epoch 8 Batch 300 Loss 0.5216\n",
            "Epoch 8 Loss 0.5634 Validation Loss 1.0426\n",
            "Time taken for 1 epoch 76.99074101448059 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.5366\n",
            "Epoch 9 Batch 100 Loss 0.5489\n",
            "Epoch 9 Batch 200 Loss 0.4320\n",
            "Epoch 9 Batch 300 Loss 0.5127\n",
            "Epoch 9 Loss 0.5003 Validation Loss 0.9504\n",
            "Time taken for 1 epoch 76.89398384094238 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4154\n",
            "Epoch 10 Batch 100 Loss 0.4193\n",
            "Epoch 10 Batch 200 Loss 0.4090\n",
            "Epoch 10 Batch 300 Loss 0.3857\n",
            "Epoch 10 Loss 0.4047 Validation Loss 0.9228\n",
            "Time taken for 1 epoch 76.68457221984863 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2840\n",
            "Epoch 11 Batch 100 Loss 0.3272\n",
            "Epoch 11 Batch 200 Loss 0.3213\n",
            "Epoch 11 Batch 300 Loss 0.3248\n",
            "Epoch 11 Loss 0.3391 Validation Loss 0.9058\n",
            "Time taken for 1 epoch 83.40982937812805 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.3176\n",
            "Epoch 12 Batch 100 Loss 0.2402\n",
            "Epoch 12 Batch 200 Loss 0.3424\n",
            "Epoch 12 Batch 300 Loss 0.3060\n",
            "Epoch 12 Loss 0.2867 Validation Loss 0.8895\n",
            "Time taken for 1 epoch 76.83584523200989 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2817\n",
            "Epoch 13 Batch 100 Loss 0.2310\n",
            "Epoch 13 Batch 200 Loss 0.2255\n",
            "Epoch 13 Batch 300 Loss 0.3505\n",
            "Epoch 13 Loss 0.2415 Validation Loss 0.8879\n",
            "Time taken for 1 epoch 76.88733959197998 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2245\n",
            "Epoch 14 Batch 100 Loss 0.1563\n",
            "Epoch 14 Batch 200 Loss 0.1637\n",
            "Epoch 14 Batch 300 Loss 0.1960\n",
            "Epoch 14 Loss 0.2053 Validation Loss 0.8797\n",
            "Time taken for 1 epoch 76.75162434577942 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2120\n",
            "Epoch 15 Batch 100 Loss 0.1887\n",
            "Epoch 15 Batch 200 Loss 0.1927\n",
            "Epoch 15 Batch 300 Loss 0.2159\n",
            "Epoch 15 Loss 0.1980 Validation Loss 0.8863\n",
            "Time taken for 1 epoch 76.87544465065002 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1439\n",
            "Epoch 16 Batch 100 Loss 0.2006\n",
            "Epoch 16 Batch 200 Loss 0.2011\n",
            "Epoch 16 Batch 300 Loss 0.1657\n",
            "Epoch 16 Loss 0.1609 Validation Loss 0.8769\n",
            "Time taken for 1 epoch 76.75966787338257 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1346\n",
            "Epoch 17 Batch 100 Loss 0.1770\n",
            "Epoch 17 Batch 200 Loss 0.1490\n",
            "Epoch 17 Batch 300 Loss 0.1521\n",
            "Epoch 17 Loss 0.1493 Validation Loss 0.8903\n",
            "Time taken for 1 epoch 76.820476770401 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1012\n",
            "Epoch 18 Batch 100 Loss 0.0969\n",
            "Epoch 18 Batch 200 Loss 0.1614\n",
            "Epoch 18 Batch 300 Loss 0.0966\n",
            "Epoch 18 Loss 0.1265 Validation Loss 0.8949\n",
            "Time taken for 1 epoch 76.71603512763977 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1097\n",
            "Epoch 19 Batch 100 Loss 0.1310\n",
            "Epoch 19 Batch 200 Loss 0.0797\n",
            "Epoch 19 Batch 300 Loss 0.1192\n",
            "Epoch 19 Loss 0.1110 Validation Loss 0.8878\n",
            "Time taken for 1 epoch 76.91186189651489 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0984\n",
            "Epoch 20 Batch 100 Loss 0.0953\n",
            "Epoch 20 Batch 200 Loss 0.1110\n",
            "Epoch 20 Batch 300 Loss 0.1389\n",
            "Epoch 20 Loss 0.0967 Validation Loss 0.9040\n",
            "Time taken for 1 epoch 76.77867674827576 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "\n",
        "attention = BahdanauAttention(units)\n",
        "print(\"Running seq2seq model with Bahdanau attention\")\n",
        "encoder_bah, decoder_bah, training_loss, validation_loss = training_seq2seq(epochs, attention)\n",
        "\n",
        "tloss = np.vstack((tloss, training_loss))\n",
        "vloss = np.vstack((vloss, validation_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXIb4R_-q-rW",
        "outputId": "8485c9c4-06fa-496d-8825-f926da552f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running seq2seq model with Bahdanau attention\n",
            "Epoch 1 Batch 0 Loss 7.7280\n",
            "Epoch 1 Batch 100 Loss 3.1505\n",
            "Epoch 1 Batch 200 Loss 2.6946\n",
            "Epoch 1 Batch 300 Loss 2.6451\n",
            "Epoch 1 Loss 3.1679 Validation Loss 2.4052\n",
            "Time taken for 1 epoch 95.46813035011292 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.2939\n",
            "Epoch 2 Batch 100 Loss 2.2124\n",
            "Epoch 2 Batch 200 Loss 2.0370\n",
            "Epoch 2 Batch 300 Loss 1.9225\n",
            "Epoch 2 Loss 2.1305 Validation Loss 2.0288\n",
            "Time taken for 1 epoch 78.92238354682922 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.9779\n",
            "Epoch 3 Batch 100 Loss 1.9651\n",
            "Epoch 3 Batch 200 Loss 1.8655\n",
            "Epoch 3 Batch 300 Loss 1.7625\n",
            "Epoch 3 Loss 1.7708 Validation Loss 1.7417\n",
            "Time taken for 1 epoch 78.77114820480347 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.4712\n",
            "Epoch 4 Batch 100 Loss 1.5739\n",
            "Epoch 4 Batch 200 Loss 1.5956\n",
            "Epoch 4 Batch 300 Loss 1.4949\n",
            "Epoch 4 Loss 1.5103 Validation Loss 1.5628\n",
            "Time taken for 1 epoch 78.99677038192749 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.3392\n",
            "Epoch 5 Batch 100 Loss 1.3150\n",
            "Epoch 5 Batch 200 Loss 1.2526\n",
            "Epoch 5 Batch 300 Loss 1.1983\n",
            "Epoch 5 Loss 1.3023 Validation Loss 1.4355\n",
            "Time taken for 1 epoch 78.97878456115723 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.2104\n",
            "Epoch 6 Batch 100 Loss 1.1287\n",
            "Epoch 6 Batch 200 Loss 1.0699\n",
            "Epoch 6 Batch 300 Loss 1.1210\n",
            "Epoch 6 Loss 1.1068 Validation Loss 1.2782\n",
            "Time taken for 1 epoch 78.86120438575745 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9926\n",
            "Epoch 7 Batch 100 Loss 0.9765\n",
            "Epoch 7 Batch 200 Loss 0.8874\n",
            "Epoch 7 Batch 300 Loss 0.8775\n",
            "Epoch 7 Loss 0.9342 Validation Loss 1.1667\n",
            "Time taken for 1 epoch 78.66922736167908 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8671\n",
            "Epoch 8 Batch 100 Loss 0.8181\n",
            "Epoch 8 Batch 200 Loss 0.7985\n",
            "Epoch 8 Batch 300 Loss 0.7852\n",
            "Epoch 8 Loss 0.7826 Validation Loss 1.0682\n",
            "Time taken for 1 epoch 78.81675219535828 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.6439\n",
            "Epoch 9 Batch 100 Loss 0.6966\n",
            "Epoch 9 Batch 200 Loss 0.6995\n",
            "Epoch 9 Batch 300 Loss 0.7113\n",
            "Epoch 9 Loss 0.6486 Validation Loss 1.0138\n",
            "Time taken for 1 epoch 78.78405976295471 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4849\n",
            "Epoch 10 Batch 100 Loss 0.4615\n",
            "Epoch 10 Batch 200 Loss 0.5637\n",
            "Epoch 10 Batch 300 Loss 0.5179\n",
            "Epoch 10 Loss 0.5388 Validation Loss 0.9384\n",
            "Time taken for 1 epoch 78.72267556190491 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.5009\n",
            "Epoch 11 Batch 100 Loss 0.5140\n",
            "Epoch 11 Batch 200 Loss 0.4401\n",
            "Epoch 11 Batch 300 Loss 0.4107\n",
            "Epoch 11 Loss 0.4333 Validation Loss 0.8954\n",
            "Time taken for 1 epoch 78.91342973709106 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.3293\n",
            "Epoch 12 Batch 100 Loss 0.3353\n",
            "Epoch 12 Batch 200 Loss 0.3339\n",
            "Epoch 12 Batch 300 Loss 0.4445\n",
            "Epoch 12 Loss 0.3492 Validation Loss 0.8644\n",
            "Time taken for 1 epoch 78.62454533576965 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.3068\n",
            "Epoch 13 Batch 100 Loss 0.2730\n",
            "Epoch 13 Batch 200 Loss 0.3684\n",
            "Epoch 13 Batch 300 Loss 0.3077\n",
            "Epoch 13 Loss 0.2795 Validation Loss 0.8482\n",
            "Time taken for 1 epoch 78.65948724746704 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1762\n",
            "Epoch 14 Batch 100 Loss 0.2425\n",
            "Epoch 14 Batch 200 Loss 0.2603\n",
            "Epoch 14 Batch 300 Loss 0.2061\n",
            "Epoch 14 Loss 0.2231 Validation Loss 0.8391\n",
            "Time taken for 1 epoch 78.74511957168579 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.1692\n",
            "Epoch 15 Batch 100 Loss 0.1428\n",
            "Epoch 15 Batch 200 Loss 0.1354\n",
            "Epoch 15 Batch 300 Loss 0.1911\n",
            "Epoch 15 Loss 0.1782 Validation Loss 0.8286\n",
            "Time taken for 1 epoch 78.9541220664978 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1469\n",
            "Epoch 16 Batch 100 Loss 0.1204\n",
            "Epoch 16 Batch 200 Loss 0.1212\n",
            "Epoch 16 Batch 300 Loss 0.1529\n",
            "Epoch 16 Loss 0.1415 Validation Loss 0.8368\n",
            "Time taken for 1 epoch 78.71201157569885 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0863\n",
            "Epoch 17 Batch 100 Loss 0.0839\n",
            "Epoch 17 Batch 200 Loss 0.1006\n",
            "Epoch 17 Batch 300 Loss 0.1387\n",
            "Epoch 17 Loss 0.1132 Validation Loss 0.8341\n",
            "Time taken for 1 epoch 83.84819912910461 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0863\n",
            "Epoch 18 Batch 100 Loss 0.0835\n",
            "Epoch 18 Batch 200 Loss 0.0957\n",
            "Epoch 18 Batch 300 Loss 0.0944\n",
            "Epoch 18 Loss 0.0914 Validation Loss 0.8420\n",
            "Time taken for 1 epoch 78.7323203086853 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0586\n",
            "Epoch 19 Batch 100 Loss 0.0511\n",
            "Epoch 19 Batch 200 Loss 0.0780\n",
            "Epoch 19 Batch 300 Loss 0.0683\n",
            "Epoch 19 Loss 0.0755 Validation Loss 0.8510\n",
            "Time taken for 1 epoch 78.53919577598572 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0568\n",
            "Epoch 20 Batch 100 Loss 0.0510\n",
            "Epoch 20 Batch 200 Loss 0.0607\n",
            "Epoch 20 Batch 300 Loss 0.0675\n",
            "Epoch 20 Loss 0.0646 Validation Loss 0.8616\n",
            "Time taken for 1 epoch 78.52779603004456 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax = plt.subplot(111)\n",
        "t = np.arange(1, epochs+1)\n",
        "\n",
        "for i in range(0, vloss.shape[0]):\n",
        "  line, = plt.plot(t, vloss[i,:], lw=2)\n",
        "\n",
        "ax.legend(('No attention', 'Dot product', 'Bahdanau'))\n",
        "ax.set_title(\"Validation loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "lWQ_CDy-tFnP",
        "outputId": "65e77e75-6137-468c-f47d-0eb0db775271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Validation loss')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VptgwfvfviQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translate"
      ],
      "metadata": {
        "id": "tYMrfnnPvxMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, encoder, decoder):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp, padding='post')\n",
        "\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1,units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "  return result, sentence"
      ],
      "metadata": {
        "id": "NLt61GKbvyJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, sentence = translate(u'esta es mi vida.', encoder_bah, decoder_bah)\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBYbLOjrxqAq",
        "outputId": "c8199a06-4e05-4a36-fcfc-e26a283f0b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my life . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result, sentence = translate(u'esta es mi vida.', encoder_dp, decoder_dp)\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZhWsnCXxr0Z",
        "outputId": "4f91f83b-f578-41b3-fea6-33a6eed03c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my life . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result, sentence = translate(u'¿todavia estan en casa?', encoder_bah, decoder_bah)\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYe5O4I0x09N",
        "outputId": "4d73bb5d-98a8-462f-e5a9-d90bbdcd52b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> ¿ todavia estan en casa ? <end>\n",
            "Predicted translation: are you still at home ? <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result, sentence = translate(u'tengo un gato negro.', encoder_dp, decoder_dp)\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "metadata": {
        "id": "boU-vRd0x3qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe1f6bf6-ec4e-40cb-b5c1-3c11d5ebad85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> tengo un gato negro . <end>\n",
            "Predicted translation: i have a black cat . <end> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W0fJ2nIyPw3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}